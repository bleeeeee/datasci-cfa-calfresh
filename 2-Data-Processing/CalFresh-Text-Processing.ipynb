{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Spell Corrections for Cal-Fresh Application Dataset\n",
    "\n",
    "Author: Rocio Ng (DSWG Lead)\n",
    "\n",
    "### Summary:  \n",
    "* The purpose of this notebook is to test various methods for spell checking/correcting free text entered into Applications for the CalFresh Program (https://www.getcalfresh.org/)\n",
    "\n",
    "### Resources:\n",
    "* Spanish Language Corpus - https://www.corpusdata.org/spanish.asp\n",
    "\n",
    "Eric's notes:\n",
    "replaced i'm with i am\n",
    "replaced ' (most often in contractions) with \"\"\n",
    "replaced all other punctuation with \" \" \n",
    "then replaced all double spaces with single\n",
    "\n",
    "do we need to add whitelists to enchant dictionary?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once') # displays warnings only once\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# For loading Helper Functions\n",
    "module_path = os.path.abspath(os.path.join('helper-modules'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# For Multicore processing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Helper Modules\n",
    "from spell_checking_functions import *\n",
    "from text_processing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing spell checker\n",
    "\n",
    "correction_phrase(\"helpp meh with calfrsh whil i'm applying for ssi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detect('Hi')) # False Negative Results\n",
    "print(detect('I currently live in my truck'))\n",
    "print(detect(\"estoy embarazada\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "* Make sure paths point to where data files are stored locally if you choose to rename/move things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(\"1-Data-Files/500_sample_results.csv\")\n",
    "# text_df = pd.read_csv(\"1-Data-Files/orig_entRep_300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "* Light text processing\n",
    "* Detect Langage\n",
    "* Count Spelling Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo \n",
    "initial_phrase_processing(\" PERSON wenT to The Store at CARDINAL place!!!\")\n",
    "initial_phrase_processing(\" PERSON wenT to The Store at CARDINAL!!!\") # Entities need to be reformatted to have whitespace surrounding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['processed_phrase'] = text_df.with_entity_replacement\\\n",
    "    .apply(lambda x: initial_phrase_processing(x))\n",
    "\n",
    "text_df['language'] = text_df.with_entity_replacement\\\n",
    "    .apply(lambda x: detect_B(x)) # using modified version of the built-in detect function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['spelling_errors'] = text_df.processed_phrase\\\n",
    "    .apply(lambda x: check_phrase(x))\n",
    "\n",
    "text_df = text_df.sort_values('spelling_errors', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.groupby(by = \"language\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_languages = text_df[text_df.language.isin(['None'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Spell Checking Functions\n",
    "\n",
    "* Convert Dataframe column of Phrases to List to enable Multiprocessing\n",
    "* Run spell Correction_phrase function on text\n",
    "* Append back to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['spelling_corrections'] = text_df.apply(lambda )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_error_list = text_df['processed_phrase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_error_list = text_df[['processed_phrase', 'language']].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_error_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "spelling_error_list[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pool = Pool(processes=4) # change to number of cores in machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction(\"tte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spelling_corrections = my_pool.map(correction_phrase, spelling_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_corrections[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['spelling_corrections'] = spelling_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = text_df.iloc[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(\"gcf_circumstances_spell_correct.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load White List Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For converting text to CSV, preferable \n",
    "# import csv \n",
    "\n",
    "# txt_file = r\"2-Whitelist-Docs/wordsEn.txt\"\n",
    "# csv_file = r\"2-Whitelist-Docs/wordsEn.csv\"\n",
    "\n",
    "# in_txt = csv.reader(open(txt_file, \"rb\"), delimiter = '\\t')\n",
    "# out_csv = csv.writer(open(csv_file, 'wb'))\n",
    "\n",
    "# out_csv.writerows(in_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_df1 = pd.read_csv(\"2-Whitelist-Docs/white-list.csv\")\n",
    "whitelist_df2 = pd.read_csv(\"2-Whitelist-Docs/wordsEn.csv\")\n",
    "whitelist_list1 = whitelist_df1[\"word\"].tolist()\n",
    "whitelist_list2 = whitelist_df2[\"word\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"part\" in whitelist_list1\n",
    "\"part\" in whitelist_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_list = whitelist_list1 + whitelist_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"This is a Test.   For Rocio. Hello. \"\n",
    "def check_whitelist(phrase, whitelist, method = \"remove\"):\n",
    "    \n",
    "    # clean phrase and split into words\n",
    "    phrase = phrase.lower().strip()\n",
    "    phrase = removePunctuation(phrase)\n",
    "    word_list = phrase.split(\" \")\n",
    "    \n",
    "    # word_list = re.findall(r\"[\\w']+|[.,!?;]\", phrase)\n",
    "    word_list = [word for word in word_list if word != \"\" ]  # for clearing double spaces\n",
    "\n",
    "    # remove words not in white list and calculate # of words removed\n",
    "    if method == \"remove\":\n",
    "        cleaned_word_list = [word for word in word_list if word in whitelist]\n",
    "        delta = len(word_list) - len(cleaned_word_list)\n",
    "        cleaned_phrase = \" \".join(cleaned_word_list)\n",
    "    elif method == \"replace\":\n",
    "        cleaned_word_list = [word if word in whitelist else \"[redacted]\" for word in word_list]\n",
    "        cleaned_phrase = \" \".join(cleaned_word_list)\n",
    "        delta = cleaned_word_list.count(\"[redacted]\")\n",
    "    return cleaned_phrase, delta\n",
    "\n",
    "\n",
    "def removed_words(phrase, whitelist):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_whitelist(test_phrase, whitelist_list, \"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['whitelisted_phrase'] = text_df.spelling_corrections\\\n",
    "    .apply(lambda x: check_whitelist(x, whitelist_list, \"replace\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"whitelisted_phrase\"].to_csv(\"cleant_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(\"gcf_circumstances_spell_correct_whitelist_300.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Effectiveness of Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_df['words_removed_raw_words'] = text_df.original_additional_information_text\\\n",
    "#     .apply(lambda x: int(check_whitelist(x, whitelist_list)[1]))\n",
    "\n",
    "# text_df['words_removed_spell_corrected'] = text_df.spelling_corrections\\\n",
    "#     .apply(lambda x: int(check_whitelist(x, whitelist_list)[1]))\n",
    "\n",
    "text_df = text_df\\\n",
    "    .assign(pct_improvement = 100*(1 - (text_df.words_removed_spell_corrected/text_df.words_removed_raw_words)))\\\n",
    "    .assign(improvement = text_df.words_removed_raw_words - text_df.words_removed_spell_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = [\"test\", \"in\", \"an\", \"never\", \"work\", \"part\", \"house\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline to beat 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Other Versions of the SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellcheck_v2_RN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer.Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer_exceptions import BASE_EXCEPTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This sentence ain't gonna be ORG grammatically correct7. 9 >>{:o) THis sentence about SAR7 PERSON doesn't have mispeled wordz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
